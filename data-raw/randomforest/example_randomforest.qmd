---
title:        Random forests examples
subtitle:     randomForestSRC
author:       Erik Erhardt
date:         last-modified # today, now, last-modified
date-format:  long          # full, long, medium, short, iso,   https://quarto.org/docs/reference/dates.html
format:
  html:
    theme:                  litera
    highlight-style:        atom-one
    page-layout:            full      # article, full   # https://quarto.org/docs/output-formats/page-layout.html
    toc:                    true
    toc-depth:              4
    toc-location:           body      # left, body, right
    number-sections:        true      # true, false
    number-depth:           3
    code-fold:              show      # true (initially hidden), false, show (initially shown)
    code-tools:                       # menu top-right to show/hide all code
      toggle:               true
      caption:              "Code"    # none
      source:               false
    code-overflow:          scroll    # scroll, wrap
    code-block-bg:          true
    code-block-border-left: "#30B0E0"
    df-print:               paged     # default, kable, tibble, paged   # https://quarto.org/docs/computations/r.html
fig-width:                  6
fig-height:                 4
execute: # https://quarto.org/docs/computations/execution-options.html, https://quarto.org/docs/computations/r.html
  cache:    false   # false, true
  eval:     true    # true, false  Evaluate the code chunk (if false, just echos the code into the output).
  echo:     true    # true, false  Include the source code in output
---

<!---
# Erik's compile commands in R:
  fn_qmd <- "example_randomforest.qmd"
  setwd("D:/Dropbox/StatAcumen/consult/Rpackages/erikmisc/data-raw/randomforest")
  quarto::quarto_render(input = fn_qmd)
-->

```{r chunk-01, echo=FALSE}
options(width = 80)
#options(warn  = -1)
options(str   = strOptions(list.len = 1e3))
options(knitr.kable.NA = '') # Display NAs as blanks
my_seed <- 34567
set.seed(my_seed)
```

# Examples

## Load standard packages

```{r}
library(erikmisc)
library(tidyverse)
library(randomForestSRC)

# Parallel processing
library(parallel)
options(rf.cores = parallel::detectCores() - 2) # OpenMP Parallel Processing
options(mc.cores = parallel::detectCores() - 2) # R-side Parallel Processing
```

## Timing

```{r}
# install.packages("tictoc")
tictoc::tic(msg = "Timer")
tictoc::toc()
```


---

# Erik's Vignette

* https://www.randomforestsrc.org/articles/getstarted.html
* https://www.randomforestsrc.org/cheatsheets.pdf

## Preparation

### Clean up and impute data

```{r}
#| fig-width:  10
#| fig-height: 8

# Dataset needs to only have numeric and factor variables, no character columns
# Dataset needs to be a dataframe -- a tibble will terminate the R session
dat_mtcars_e <-
  erikmisc::dat_mtcars_e |>
  as.data.frame()
dat_miss_org <-
  dat_mtcars_e |>
  dplyr::select(-model)
dat_miss <- dat_miss_org

prop_missing <- 0.10
n_missing <- sample.int(n = prod(dim(dat_miss)), size = round( prop_missing * prod(dim(dat_miss))))
ind_missing <- expand.grid(1:dim(dat_miss)[1], 1:dim(dat_miss)[2])[n_missing, ]
for (i_row in seq_along(n_missing)) {
  dat_miss[ind_missing[i_row,1], ind_missing[i_row,2] ] <- NA
}


dat_miss <- dat_miss |> as.data.frame()

## Fast imputation mode is less accurate
#     uses randomForestSRC::impute()
dat_miss_impute_fast <-
  randomForestSRC::impute(
  # mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb
    data = dat_miss
  )

# Data frame differences between original and imputed
#diffdf::diffdf(dat_miss_org, dat_miss_impute_fast)



## Slow imputation mode is more accurate
#    uses randomForestSRC::rfsrc(na.action = "na.impute")
dat_miss_impute_obj <-
  randomForestSRC::rfsrc(
  # mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb
    data = dat_miss
  , na.action = "na.impute"
  )

ind_row_impute <-
  which(!complete.cases(dat_miss_impute_obj$xvar))

dat_miss_impute_slow <-
  dat_miss_impute_obj$xvar
dat_miss_impute_slow[ ind_row_impute, ] <-
  dat_miss_impute_obj$imputed.data

dat_miss_impute_slow

# Data frame differences between original and imputed
#diffdf::diffdf(dat_miss_org, dat_miss_impute_slow)


library(patchwork)
e_plot_missing(dat_miss_org) +
  e_plot_missing(dat_miss) +
  e_plot_missing(dat_miss_impute_fast) +
  e_plot_missing(dat_miss_impute_slow)
```

### Prepare data and formula

```{r}
# keep only variables for analysis, y as first variable, as.data.frame()
dat_rf_data <-
  dat_mtcars_e |>
  dplyr::select(
    -model
  ) |>
  dplyr::select(
    cyl
  , tidyselect::everything()
  ) |>
  as.data.frame()

rf_y_var <- names(dat_rf_data)[ 1]
rf_x_var <- names(dat_rf_data)[-1]

# After variable selection
rf_x_var <-
  c(
    "mpg"
  , "disp"
  , "hp"
  #, "drat"
  , "wt"
  #, "qsec"
  , "vs"
  #, "am"
  #, "gear"
  , "carb"
  )

rf_formula <-
  paste(
    rf_y_var
  , " ~ "
  , paste(
      rf_x_var
    , collapse= " + "
    )
  ) |>
  as.formula()

dat_rf_data |> str()
rf_y_var
rf_x_var
rf_formula
```

### Tune mtry and nodesize

Sometimes fails.

```{r}
# repeat several times and watch the surface change substantialy
o <-
  randomForestSRC::tune(
    formula = rf_formula
  , data    = dat_rf_data
  )
o$optimal

## visualize the nodesize/mtry OOB surface

## nice little wrapper for plotting results
plot.tune <- function(o, linear = TRUE) {
  x <- o$results[,1]
  y <- o$results[,2]
  z <- o$results[,3]
  so <- interp::interp(x=x, y=y, z=z, linear = linear)
  idx <- which.min(z)
  x0 <- x[idx]
  y0 <- y[idx]
  filled.contour(
    x = so$x
  , y = so$y
  , z = so$z
  , xlim = range(so$x, finite = TRUE) + c(-2, 2)
  , ylim = range(so$y, finite = TRUE) + c(-2, 2)
  , color.palette = colorRampPalette(c("yellow", "red"))
  , xlab = "nodesize"
  , ylab = "mtry"
  , main = "error rate for nodesize and mtry"
  , key.title = title(main = "OOB error", cex.main = 1)
  , plot.axes = {axis(1);axis(2);points(x0,y0,pch="x",cex=1,font=2);
                points(x,y,pch=16,cex=.25)}
  )
}

## plot the surface
plot.tune(o)

```


----------------------------------------

## Classification, erikmisc function with automated selection

### Classification

```{r}
### Prepare data and formula

# keep only variables for analysis, y as first variable, as.data.frame()
dat_rf_class <-
  erikmisc::dat_mtcars_e |>
  dplyr::select(
    -model
  ) |>
  dplyr::select(
    cyl
  , tidyselect::everything()
  )

#rf_y_var <- "cyl"
#rf_x_var <- c("mpg", "disp", "hp", "drat", "wt", "qsec", "vs", "am", "gear", "carb")


out_e_rf <-
  e_rfsrc_classification(
    dat_rf_class    = dat_rf_class
  , rf_y_var        = NULL  # if not specified, assumed to be first column
  , rf_x_var        = NULL  # if not specified, assumed to be second through last columns
  , sw_rfsrc_ntree  = 1000
  , sw_alpha        = 0.05  # for subsample bootsample model selection and VIMP plot
  )

```


### Prediction

```{r}

out_e_rf$o_class_sel

dat_rf_class_to_pred <-
  dat_rf_class |>
  dplyr::mutate(
    cyl_org = cyl
  , cyl = NA
  )

out_e_rf_pred <-
  predict(
    object        = out_e_rf$o_class_sel
  , newdata       = dat_rf_class_to_pred # dat_rf_class  #
  )
  ### Other prediction arguments
  # #, m.target      = NULL
  # , importance  = c("anti", "permute", "random", "none")[1]
  # #, get.tree = NULL
  # , block.size  = 1 # 10 #if (any(is.element(as.character(importance), c("none", "FALSE")))) NULL else 10
  # , na.action   = c("na.omit", "na.impute")[2]
  # , outcome = c("train", "test")[2]
  # , perf.type   = c("none", "misclass", "brier", "gmean")[2]  # classification
  # , proximity   = c("inbag", "oob", "all", TRUE, FALSE)[2]    # TRUE = "inbag"
  # , distance    = c("inbag", "oob", "all", TRUE, FALSE)[2]    # TRUE = "inbag"
  # , forest.wt   = c("inbag", "oob", "all", TRUE, FALSE)[1]    # TRUE = "inbag"
  # #, ptn.count = 0
  # , var.used    = c(FALSE, "all.trees", "by.tree")[2]
  # , split.depth = c(FALSE, "all.trees", "by.tree")[2]
  # #, seed = NULL
  # , do.trace    = 1
  # , statistics  = TRUE # FALSE
  # #, membership = FALSE
  #)

out_e_rf_pred$predicted
out_e_rf_pred$class

dat_rf_class_to_pred

dat_rf_class_to_pred <-
  dat_rf_class_to_pred |>
  dplyr::bind_cols(
    out_e_rf_pred$predicted
  , tibble::tibble(class = out_e_rf_pred$class)
  ) |>
  dplyr::select(
  #  ID
    class
  , tidyselect::all_of(colnames(out_e_rf_pred$predicted))
  , tidyselect::everything()
  )


```

--------------------------------------------------------------------------------

## Classification

All the individual steps to learn how it all works.

### Grow forest

```{r}
tictoc::tic(msg = "Timer")

o_class <-
  randomForestSRC::rfsrc(
    formula     = rf_formula
  , data        = dat_rf_data
  , ntree       = 2000
  #, mtry        = NULL             # tune()$optimal["mtry"]
  #, ytry        = NULL
  #, nodesize    = NULL             # tune()$optimal["nodesize"]
  #, nodedepth   = NULL
  #, splitrule   = NULL
  #, splitrule   = c("mse", "quantile.regr")[1]       # regression
  , splitrule   = c("gini", "auc", "entropy")[2]     # classification
  , nsplit      = 0 # 10
  , importance  = c("anti", "permute", "random", "none")[1]
                    # "permute" yields permutation VIMP (Breiman-Cutler importance) by permuting OOB cases.
                    # "random"  uses random left/right assignments whenever a split is encountered for the target variable.
                    # "anti"    (default) assigns cases to the anti (opposite) split.
  , block.size  = 1 # 10 #if (any(is.element(as.character(importance), c("none", "FALSE")))) NULL else 10
  , bootstrap   = c("by.root", "none", "by.user")[1]
  , samptype    = c("swor", "swr")[1]
  #, samp        = NULL
  #, membership  = FALSE
  #, sampsize    = if (samptype == "swor") function(x){x * .632} else function(x){x}
  , na.action   = c("na.omit", "na.impute")[2]
  #, nimpute     = 1
  #, ntime       = 150    # survival
  #, cause       = NULL   # competing risks
  , perf.type   = c("none", "misclass", "brier", "gmean")[2]  # classification
  , proximity   = c("inbag", "oob", "all", TRUE, FALSE)[2]    # TRUE = "inbag"
  , distance    = c("inbag", "oob", "all", TRUE, FALSE)[2]    # TRUE = "inbag"
  , forest.wt   = c("inbag", "oob", "all", TRUE, FALSE)[1]    # TRUE = "inbag"
  #, xvar.wt     = NULL
  #, yvar.wt     = NULL
  #, split.wt    = NULL
  #, case.wt     = NULL
  , forest      = TRUE
  #, save.memory = FALSE
  , var.used    = c(FALSE, "all.trees", "by.tree")[2]
  , split.depth = c(FALSE, "all.trees", "by.tree")[2]
  #, seed        = NULL
  , do.trace    = 1
  , statistics  = TRUE # FALSE
  )

tictoc::toc()

o_class
```

### Plot convergence

```{r}
#| fig-width:  10
#| fig-height: 8

plot(o_class)
```

### OOB Classification accuracy

```{r}
# OOB Classification accuracy
#o_class$class.oob           # OOB predicted class labels.
#o_class$predicted.oob       # Out-of-bag (OOB) predicted values for the training dataset
out_pred <-
  data.frame(
    id        = dat_rf_data |> rownames() |> as.numeric()
  , y_var_org = dat_rf_data[[ rf_y_var ]]
  , class_oob = o_class$class.oob
  , prob      = o_class$predicted.oob
  ) |>
  dplyr::mutate(
    correct = ifelse(y_var_org == class_oob, TRUE, FALSE)
  ) |>
  dplyr::arrange(
    y_var_org
  , class_oob
  , id
  )
out_pred |> print()
```

### Variable Importance (also for variable selection)

```{r}
# VIMP
o_class$importance            # Variable importance (VIMP) for each x-variable.

e_plot_rf_vimp(o_class$importance)

# VIMP using brier prediction error
vimp(o_class, perf.type = "brier")$importance

e_plot_rf_vimp(vimp(o_class, perf.type = "brier")$importance)
```

### Confidence intervals and standard errors for VIMP (variable importance)

```{r}
o_class_subsample <-
  randomForestSRC::subsample(
    obj               = o_class
  , B                 = 100           # Number of subsamples (or number of bootstraps).
  , block.size        = 1             # 1 = VIMP is calculated for each tree. "ntree" =  VIMP is calculated for the entire forest, ensemble VIMP.
  , importance        = c("anti", "permute", "random")[1]
  , subratio          = NULL
  , stratify          = TRUE
  , performance       = TRUE # FALSE  # Generalization error? User can also request standard error and confidence regions for generalization error.
  , performance.only  = FALSE         # Only calculate standard error and confidence region for the generalization error (no VIMP).
  , joint             = FALSE         # Joint VIMP for all variables? Users can also request joint VIMP for specific variables using xvar.names.
  , xvar.names        = NULL          # Specifies variables for calculating joint VIMP. By default all variables are used.
  , bootstrap         = TRUE # FALSE  # Use double bootstrap approach in place of subsampling? Much slower, but potentially more accurate.
  , verbose           = TRUE          # Provide verbose output?
  )
o_class_subsample

randomForestSRC::extract.subsample(o_class_subsample)

# Use double bootstrap approach in place of subsampling? Much slower, but potentially more accurate.
randomForestSRC::extract.bootsample(o_class_subsample)
```

### VIMP plot out to a selected alpha level

```{r}
#| fig-width:  10
#| fig-height: 8

randomForestSRC::plot.subsample(
    x           = o_class_subsample
  , alpha       = 0.01
  #, xvar.names
  , standardize = TRUE
  , normal      = TRUE
  , jknife      = FALSE
  #, target
  , m.target    = NULL
  , pmax        = 75
  , main        = ""
  , sorted      = TRUE
  )

# XXX Find output for this to create ggplot version
```

### Minimal depth for variable selection (possibly not as good as subsample)

```{r}
## Variable Selection
# Variable selection using minimal depth.

o_class_var_select <-
  randomForestSRC::var.select(
  #  formula       =
  #, data          =
    object        = o_class
  #, cause                          # competing risks
  #, m.target                       # multivariate families
  , method        = c("md", "vh", "vh.vimp")[1]
  , conservative  = c("medium", "low", "high")[1]
  #, ntree         = (if (method == "md") 1000 else 500)
  #, mvars         = (if (method != "md") ceiling(ncol(data)/5) else NULL)
  #, mtry          = (if (method == "md") ceiling(ncol(data)/3) else NULL)
  , nodesize      = 2
  #, splitrule     = NULL
  , nsplit        = 10
  #, xvar.wt       = NULL
  #, refit         = (method != "md")
  #, fast          = FALSE
  , na.action     = c("na.omit", "na.impute")[2]
  #, always.use    = NULL
  , nrep          = 50
  , K             = 5
  , nstep         = 1
  #, prefit        = list(
  #                    action = (method != "md")
  #                  , ntree = 100
  #                  , mtry = 500
  #                  , nodesize = 3
  #                  , nsplit = 1
  #                  )
  , verbose       = TRUE
  , block.size    = 10
  , seed          = NULL
  , importance    = c("permute", "random", "anti", "permute.ensemble", "random.ensemble", "anti.ensemble")[3]
  )

o_class_var_select
```

### Marginal/Partial effects plots

```{r}
#| fig-width:  10
#| fig-height: 8

## Marginal/Partial effects plots -- do for each target class
for (i_level in seq_along(levels(o_class$class))) {
  # Marginal effects plots
  randomForestSRC::plot.variable(
      x               = o_class
    #, xvar.names
    , target          = levels(o_class$class)[i_level]   # classification: first event type
    #, m.target        = NULL
    #, time
    #, surv.type       = c("mort", "rel.freq", "surv", "years.lost", "cif", "chf")
    , class.type      = c("prob", "bayes")[1]
    , partial         = FALSE  # FALSE = Marginal plots, TRUE = Partial plots
    , oob             = TRUE
    , show.plots      = TRUE
    , plots.per.page  = 4
    , granule         = 5
    , sorted          = FALSE #TRUE
    #, nvar
    , npts            = 25
    , smooth.lines    = FALSE  # when partial = TRUE, use lowess smoothed lines
    #, subset
    , main            = paste0("Marginal plot, target: ", levels(o_class$class)[i_level])
    )
}

for (i_level in seq_along(levels(o_class$class))) {
  # Partial effects plots
  randomForestSRC::plot.variable(
      x               = o_class
    #, xvar.names
    , target          = levels(o_class$class)[i_level]   # classification: first event type
    #, m.target        = NULL
    #, time
    #, surv.type       = c("mort", "rel.freq", "surv", "years.lost", "cif", "chf")
    , class.type      = c("prob", "bayes")[1]
    , partial         = TRUE # FALSE = Marginal plots, TRUE = Partial plots
    , oob             = TRUE
    , show.plots      = TRUE
    , plots.per.page  = 4
    , granule         = 5
    , sorted          = FALSE #TRUE
    #, nvar
    , npts            = 25
    , smooth.lines    = FALSE  # when partial = TRUE, use lowess smoothed lines (too smooth)
    #, subset
    , main            = paste0("Partial plot, target: ", levels(o_class$class)[i_level])
    )
}
```

### Variable interaction with maxsubtree

```{r}
#| fig-width:  10
#| fig-height: 8

# Interactions
o_class_interaction_maxsubtree <-
  randomForestSRC::find.interaction(
    object      = o_class
  #, xvar.names
  #, cause
  #, m.target
  , importance  = c("permute", "random", "anti", "permute.ensemble", "random.ensemble", "anti.ensemble")[3]
  , method      = c("maxsubtree", "vimp")[1]
                    # "maxsubtree"   Smaller [i][i] entries indicate predictive variables. Small [i][j] entries having small [i][i] entries are a sign of an interaction between variable i and j (note: the user should scan rows, not columns, for small entries). See Ishwaran et al. (2010, 2011) for more details.
                    # "vimp"         A large positive or negative difference between 'Paired' and 'Additive' indicates an association worth pursuing if the univariate VIMP for each of the paired-variables is reasonably large. See Ishwaran (2007) for more details.

  , sorted      = TRUE
  #, nvar
  , nrep        = 1
  , na.action = c("na.omit", "na.impute", "na.random")[2]
  #, seed = NULL
  #, do.trace = FALSE
  , verbose = TRUE
  )

o_class_interaction_maxsubtree

o_class_interaction_vimp <-
  randomForestSRC::find.interaction(
    object      = o_class
  #, xvar.names
  #, cause
  #, m.target
  , importance  = c("permute", "random", "anti", "permute.ensemble", "random.ensemble", "anti.ensemble")[3]
  , method      = c("maxsubtree", "vimp")[2]
                    # "maxsubtree"   Smaller [i][i] entries indicate predictive variables. Small [i][j] entries having small [i][i] entries are a sign of an interaction between variable i and j (note: the user should scan rows, not columns, for small entries). See Ishwaran et al. (2010, 2011) for more details.
                    # "vimp"         A large positive or negative difference between 'Paired' and 'Additive' indicates an association worth pursuing if the univariate VIMP for each of the paired-variables is reasonably large. See Ishwaran (2007) for more details.

  , sorted      = TRUE
  #, nvar
  , nrep        = 1
  , na.action = c("na.omit", "na.impute", "na.random")[2]
  #, seed = NULL
  #, do.trace = FALSE
  , verbose = TRUE
  )

o_class_interaction_vimp
```

### Bivariate partial plots

```{r}
#| fig-width:  10
#| fig-height: 8

## Bivariate partial plot requires more care

# specify variables
rf_bi_partial         <- list()
rf_bi_partial$xvars   <- c("mpg", "disp")
rf_bi_partial$values  <- list()
rf_bi_partial$values[[1]] <- o_class$xvar[[ rf_bi_partial$xvars[1] ]] |> unique() |> sort()
rf_bi_partial$values[[2]] <- o_class$xvar[[ rf_bi_partial$xvars[2] ]] |> unique() |> sort()

# create plot values
o_class_bi_partial_plot <-
  do.call(
    rbind
  , lapply(
      rf_bi_partial$values[[2]]  # xvar 2
    , function(x2) {
        o_bi_temp <-
          randomForestSRC::partial(
            object          = o_class
          , oob             = TRUE
          #, partial.type    = NULL
          , partial.xvar    = rf_bi_partial$xvars[1]
          , partial.values  = rf_bi_partial$values[[1]]
          , partial.xvar2   = rf_bi_partial$xvars[2]
          , partial.values2 = x2
          #, partial.time    = NULL
          #, get.tree        = NULL
          #, seed            = NULL
          #, do.trace        = FALSE
          )

        out <-
          cbind(
            rf_bi_partial$values[[1]]  # xvar 1
          , x2
          , randomForestSRC::get.partial.plot.data(o_bi_temp)$yhat
          )

        return(out)
      }
    )
  ) |>
  data.frame()
colnames(o_class_bi_partial_plot) <-
  c(
    rf_bi_partial$xvars[1]
  , rf_bi_partial$xvars[2]
  , "EffectSize"
  )

# print plots
# xvar1 | xvar2
coplot(
    formula =
      paste0(
        "EffectSize"
      , " ~ "
      , rf_bi_partial$xvars[1]
      , " | "
      , rf_bi_partial$xvars[2]
      ) |>
      as.formula()
  , o_class_bi_partial_plot
  , pch = 16
  , overlap = 0
  )

# xvar2 | xvar1
coplot(
    formula =
      paste0(
        "EffectSize"
      , " ~ "
      , rf_bi_partial$xvars[2]
      , " | "
      , rf_bi_partial$xvars[1]
      ) |>
      as.formula()
  , o_class_bi_partial_plot
  , pch = 16
  , overlap = 0
  )
```

### Acquire Maximal Subtree Information

```{r}
## Acquire Maximal Subtree Information
# Used for variable selection and identifying interactions between variables.
# The smaller the minimal depth, the more impact x has on prediction.

o_class_max_subtree <-
  randomForestSRC::max.subtree(
    object        = o_class
  , max.order     = 2
  , sub.order     = TRUE # FALSE  #  TRUE to return the minimal depth of each variable relative to another variable. Used to identify interrelationship between variables.
  , conservative  = FALSE
  )

# first and second order depths
print(round(o_class_max_subtree$order, 3))

# the minimal depth is the first order depth
print(round(o_class_max_subtree$order[, 1], 3))

# strong variables have minimal depth less than or equal
# to the following threshold
print(o_class_max_subtree$threshold)

# this corresponds to the set of variables
print(o_class_max_subtree$topvars)
```

### ROC Curve

```{r}
# obtains the value of AUC (area under the ROC curve)
o_class_AUC <-
  randomForestSRC::get.auc(
    y    = dat_rf_data[[ rf_y_var ]]
  , prob = o_class$predicted.oob
  )
o_class_AUC

# ROC via erikmisc

p_list <- list()

for (n_target in levels(dat_rf_data[[ rf_y_var ]])) {
  out <-
    e_plot_roc(
      labels_true     = ifelse(dat_rf_data[[ rf_y_var ]] == n_target, 1, 0)
    , pred_values_pos = o_class$predicted.oob[, n_target]
    , label_neg_pos   = c(0, 1)
    , sw_plot         = TRUE
    )
  out$roc_curve_best |> print(width = Inf)
  p <- out$plot_roc
  p <- p + labs(title = paste0("ROC Curve, Target:  ", n_target))
  p <- p + coord_fixed(ratio = 1) # equal axes
  p_list[[ n_target ]] <- p
  out$confusion_matrix
} # n_target

p_arranged <-
  cowplot::plot_grid(
    plotlist = p_list
  , nrow = 1
  #, ncol = 2
  )

p_arranged |> print()


# ROC via plotROC

library(plotROC)

p_list <- list()
for (n_target in levels(dat_rf_data[[ rf_y_var ]])) {
  dat_ROC <-
    tibble::tibble(
      labels_true     = ifelse(dat_rf_data[[ rf_y_var ]] == n_target, 1, 0)
    , pred_values_pos = o_class$predicted.oob[, n_target]
    )

  p <- ggplot(dat_ROC, aes(d = labels_true, m = pred_values_pos))
  p <- p + geom_roc(cutoffs.at = c(-Inf, seq(0, 1, by = 0.2), Inf))
  p <- p + style_roc(theme = theme_grey, xlab = "1 - Specificity")
  p <- p + labs(title = paste0("ROC Curve, Target: ", n_target))
  p <- p + annotate("text", x = .75, y = .25,
            label = paste("AUC =", round(plotROC::calc_auc(p)$AUC, 2)))
  p_list[[ n_target ]] <- p

  plotROC::calc_auc(p)$AUC

} # n_target

p_arranged <-
  cowplot::plot_grid(
    plotlist = p_list
  , nrow = 1
  #, ncol = 2
  )

p_arranged |> print()

#o.pred <- predict(object = o, newdata) # Predicted values for the new dataset are in o.pred$predicted
```

### ggplots

https://cran.r-project.org/web/packages/ggRandomForests/vignettes/ggrfRegression.html

```{r}




```









--------------------------------------------------------------------------------

## Regression


```
{r}




## Acquire Split Statistic Information
# https://www.randomforestsrc.org/reference/stat.split.rfsrc.html

## nice wrapper to extract split-statistic for desired variable
## for continuous variables plots ECP data
f_rf_get_split <-
  function(
    splitObj = o_class_stat_split
  , xvar     = NULL
  , inches   = 0.1
  , ...
  ) {
  which.var <- which(names(splitObj[[1]]) == xvar)
  ntree     <- length(splitObj)
  stat      <-
    data.frame(
      do.call(
        rbind
      , sapply(
          1:ntree
        , function(b) {
            splitObj[[b]][which.var]
          }
        )
      )
    )
  dpth      <- stat$dpthID
  ecp       <- 1/2 - stat$spltEC
  sp        <- stat$contPT
  if (!all(is.na(sp))) {
    fgC <- function(x) {
      as.numeric(
        as.character(
          cut(
            x
          , breaks = c(-1, 0.2, 0.35, 0.5)
          , labels = c(1, 4, 2)
          )
        )
      )
    } # fgC
    symbols(jitter(sp), jitter(dpth), ecp, inches = inches, bg = fgC(ecp),
      xlab = xvar, ylab = "node depth", ...)
    legend("topleft", legend = c("low ecp", "med ecp", "high ecp"),
      fill = c(1, 4, 2))
   }
  invisible(stat)
}

o_class_stat_split <-
  stat.split(o_class)

list_var_numeric <-
  names((unlist(lapply(dat_rf_data[,rf_x_var], class)) == "numeric")[unlist(lapply(dat_rf_data[,rf_x_var], class)) == "numeric"])

for (n_var in list_var_numeric) {
  f_rf_get_split(
      splitObj  = o_class_stat_split
    , xvar      = n_var               # numeric variables, only
    )
}

```







--------------------------------------------------------------------------------

## Clustering








--------------------------------------------------------------------------------

# Help pages

```
Help Pages

randomForestSRC             # Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)

-- DATA
breast                      # Wisconsin Prognostic Breast Cancer Data
follic                      # Follicular Cell Lymphoma
hd                          # Hodgkin's Disease
housing                     # Ames Iowa Housing Data
nutrigenomic                # Nutrigenomic Study
pbc                         # Primary Biliary Cirrhosis (PBC) Data
peakVO2                     # Systolic Heart Failure Data
vdv                         # van de Vijver Microarray Breast Cancer
wihs                        # Women's Interagency HIV Study (WIHS)
wine                        # White Wine Quality Data
veteran                     # Veteran's Administration Lung Cancer Trial

-- IMPUTE
impute                      # Impute Only Mode


-- TUNE
tune                        # Tune Random Forest for the optimal mtry and nodesize parameters
tune.nodesize               # Tune Random Forest for the optimal mtry and nodesize parameters

-- GROW
rfsrc                       # Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)

rfsrc.anonymous             # Anonymous Random Forests
rfsrc.cart                  # Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)
rfsrc.fast                  # Fast Random Forests

synthetic                   # Synthetic Random Forests
imbalanced                  # Imbalanced Two Class Problems
sidClustering               # sidClustering using SID (Staggered Interaction Data) for Unsupervised Clustering


-- VALUES                   #
o$predicted.oob             # Out-of-bag (OOB) predicted values for the training dataset
o$err.rate                  # tree cumulative OOB error rate;
print(o)                    # lists OOB error rate in the bottom;
plot(o)                     # plots OOB error rate along with number of trees;
get.auc(y, prob)            # obtains the value of AUC (area under the ROC curve)
o.pred <- predict(object = o, newdata) # Predicted values for the new dataset are in o.pred$predicted

print                       # Print Summary Output of a RF-SRC Analysis
plot                        # Plot Error Rate and Variable Importance from a RF-SRC analysis
predict                     # Prediction for Random Forests for Survival, Regression, and Classification



-- VIMP
subsample                   # Subsample Forests for VIMP Confidence Intervals
vimp                        # VIMP for Single or Grouped Variables
extract.bootsample          # Subsample Forests for VIMP Confidence Intervals
extract.subsample           # Subsample Forests for VIMP Confidence Intervals

holdout.vimp                # Hold out variable importance (VIMP)
print.bootsample            # Subsample Forests for VIMP Confidence Intervals
print.subsample             # Subsample Forests for VIMP Confidence Intervals
plot.subsample              # Plot Subsampled VIMP Confidence Intervals



-- Var selection
max.subtree                 # Acquire Maximal Subtree Information
var.select(formula, data, method) Variable selection or hunting by setting method
  md Minimal depth (default)#
  vh Variable hunting       #
  vh.vimp Variable hunting with VIMP

-- Partial Plot             #
partial                     # Acquire Partial Effect of a Variable
plot.variable               # Plot Marginal Effect of Variables
get.partial.plot.data       # Acquire Partial Effect of a Variable

find.interaction            # Find Interactions Between Pairs of Variables

get.auc                     # Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)
get.bayes.rule              # Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)
get.brier.error             # Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)
get.brier.survival          # Plot of Survival Estimates
get.cindex                  # Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)
get.confusion               # Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)
get.imbalanced.optimize     # Imbalanced Two Class Problems
get.imbalanced.performance  # Imbalanced Two Class Problems
get.misclass.error          # Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)
get.pr.auc                  # Imbalanced Two Class Problems
get.pr.curve                # Imbalanced Two Class Problems
get.rfq.threshold           # Imbalanced Two Class Problems
plot.competing.risk         # Plots for Competing Risks
plot.quantreg               # Plot Quantiles from Quantile Regression Forests
plot.survival               # Plot of Survival Estimates
sid.perf.metric             # sidClustering using SID (Staggered Interaction Data) for Unsupervised Clustering


stat.split                  # Acquire Split Statistic Information


get.tree                    # Extract a Single Tree from a Forest and plot it on your browser

-- Survival                 #

-- Quantile                 #
quantreg                    # Quantile Regression Forests
extract.quantile            # Quantile Regression Forests
get.quantile                # Quantile Regression Forests
get.quantile.crps           # Quantile Regression Forests
get.quantile.stat           # Quantile Regression Forests


-- multivariate random forest
get.mv.cserror              # Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)
get.mv.csvimp               # Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)
get.mv.error                # Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)
get.mv.error.block          # Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)
get.mv.formula              # Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)
get.mv.predicted            # Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)
get.mv.vimp                 # Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)


rfsrc.news                  # Show the NEWS file

```


# randomForestSRC notes

```{r}

library(randomForestSRC)
# New York air quality measurements. Mean ozone in parts per billion.
airq.obj <- rfsrc(Ozone ~ ., data = airquality)
print(airq.obj)

oo <- subsample(airq.obj, verbose = FALSE)
# take a delete-d-jackknife procedure for example
vimpCI <- extract.subsample(oo)$var.jk.sel.Z
vimpCI

# Confidence Intervals for VIMP
plot.subsample(oo)
# take the variable "Month" for example for partial plot
plot.variable(airq.obj, xvar.names = "Month", partial = TRUE)
```

## OOB

```{r}

## run mtcars, and print out error rate and other information
o <- rfsrc(mpg~.,mtcars)
print(o)

## we can get the error rate (mean-squared error) directly from the OOB ensemble
## by comparing the response to the OOB predictor
print(mean((o$yvar - o$predicted.oob)^2))
```

## Variable Importance (VIMP) and Dimension Reduction

```{r}
## ------------------------------------------------------------
## examples of obtaining VIMP using classification
## ------------------------------------------------------------

## directly from trained forest
rfsrc(Species~.,iris,importance=TRUE)$importance

## ... using the prediction function
o <- rfsrc(Species~.,iris)
predict(o, importance = TRUE)$importance

## ... using the vimp function
vimp(o, importance = TRUE)$importance


## ------------------------------------------------------------
## how to obtain permutation importance
## ------------------------------------------------------------

## directly from trained forest
rfsrc(Species~.,iris,importance="permute")$importance

## ... using the prediction function
o <- rfsrc(Species~.,iris)
predict(o, importance = "permute")$importance

## using the vimp function
vimp(o, importance = "permute")$importance
```

OOB Brier score and OOB AUC values:

```{r}
## ----------------------------------------------------------------
## classification analysis using Brier score for performance
## ----------------------------------------------------------------
iris.obj <- rfsrc(Species ~ ., data = iris, block.size = 1, perf.type = "brier")

## plot the error rate
plot(iris.obj)
vimp(iris.obj)$importance

get.auc(iris$Species, iris.obj$predicted.oob)
get.brier.error(iris$Species, iris.obj$predicted.oob)
```



# Vignettes
## Parallel Processing

```{r}
library(parallel)
options(rf.cores = parallel::detectCores() - 2) # OpenMP Parallel Processing
options(mc.cores = parallel::detectCores() - 2) # R-side Parallel Processing
```

## Hybrid Parallel Processing

```{r}
```

## randomForestSRC Algorithm

```{r}
```

## Speedup Random Forest Analyses

```{r}
```

## Forest Weights, In-Bag (IB) and Out-of-Bag (OOB) Ensembles

```{r}
library(randomForestSRC)
## run classification forest
o <- rfsrc(Species~.,iris)

## extract inbag and oob predictors
phat.inb <- o$predicted
phat.oob <- o$predicted.oob

phat.inb
phat.oob

## extract inbag and oob forest weights
fwt.inb <- predict(o, forest.wt="inbag")$forest.wt
fwt.oob <- predict(o, forest.wt="oob")$forest.wt

## calculate inbag and oob predictors
phat.fwt.inb <- do.call(cbind, lapply(levels(o$yvar), function(lbl) {
  apply(fwt.inb, 1, function(wt) {
    sum(wt * (o$yvar == lbl))
  })
}))
phat.fwt.oob <- do.call(cbind, lapply(levels(o$yvar), function(lbl) {
  apply(fwt.oob, 1, function(wt) {
    sum(wt * (o$yvar == lbl))
  })
}))

## show these are the same as before
print(head(data.frame(
                 IB=phat.inb,
                 OOB=phat.oob,
                 IB.fwt=phat.fwt.inb,
                 OOB.fwt=phat.fwt.oob), 20))

## notice that forest weights are convex (sum to 1)

print(rowSums(fwt.inb, na.rm = TRUE))
print(rowSums(fwt.oob, na.rm = TRUE))
```

## Random Survival Forests
(skip)

```{r}
```

## Competing Risks
(skip)

```{r}
```

## Random Forests Quantile Classifier (RFQ)
(skip)

```{r}
```

## Multivariate Splitting Rule
(skip)

```{r}
```

## AUC Splitting Rule for Multiclass Problems

```{r}
```

## sidClustering
(skip)

```{r}
```

## Variable Importance (VIMP) with Subsampling Inference

VIMP is the Breiman-Cutler importance, a permutation importance

**XXX Use standard or Brier VIMP?**

```{r}
library(randomForestSRC)
iris.obj <- rfsrc(Species ~ ., data = iris)
iris.obj
print(vimp(iris.obj)$importance)
# VIMP using brier prediction error
print(vimp(iris.obj, perf.type = "brier")$importance)

```


The following R code calculates the confidence interval using delete-d
 jackknife estimator. For .164 confidence intervals, we can use subsample(iris.obj, B = 25, bootstrap = TRUE).

```{r}
library(randomForestSRC)
iris.obj <- rfsrc(Species ~ ., data = iris)
## very small sample size so need largish subratio
reg.smp.o <- subsample(iris.obj, B = 25, subratio = .5)
## summary of results
# "parametric" refers to inference under the assumption of asymptotic normality
# "nonparametric" refers to the nonparametric subsampling method
print(reg.smp.o)


## plot confidence regions using the delete-d jackknife variance estimator
plot.subsample(reg.smp.o)
```


## Minimal Depth

variable selection approach

```
{r}
max.subtree(conservative = TRUE)

```

Second-Order Maximal Subtrees: interaction [0,1]

The matrix should be read by looking across rows (not down columns) and
identifies interrelationship between variables. Small (i,j) entries indicate
interactions.

```{r}
library("randomForestSRC")
mtcars.obj <- rfsrc(mpg ~ ., data = mtcars[, 1:7])
v.max <- max.subtree(mtcars.obj, sub.order=TRUE)
v.max$sub.order

find.interaction(mtcars.obj, method = "vimp", nrep = 3)

```



## Partial Plots

```{r}
library(randomForestSRC)
# New York air quality measurements
airq.obj <- rfsrc(Ozone ~ ., data = airquality)
#plot.variable(airq.obj, xvar.names = "Wind", partial = TRUE)
plot.variable(airq.obj, partial = TRUE)



## first run the forest
airq.obj <- rfsrc(Ozone ~ ., data = airquality)

## partial effect for wind
partial.obj <- partial(airq.obj,
                 partial.xvar = "Wind",
                 partial.values = airq.obj$xvar$Wind)

## helper function for extracting the partial effects
pdta <- get.partial.plot.data(partial.obj)

## plot partial values
plot(pdta$x, pdta$yhat, type = "b", pch = 16,
     xlab = "wind", ylab = "partial effect of wind")



## first run the forest
airq.obj <- rfsrc(Ozone ~ ., data = airquality)

## specify wind and temperature values of interest
wind <- sort(unique(airq.obj$xvar$Wind))
temp <- sort(unique(airq.obj$xvar$Temp))

## partial effect for wind, for a given temp
pdta <- do.call(rbind, lapply(temp, function(x2) {
       o <- partial(airq.obj,
             partial.xvar = "Wind", partial.xvar2 = "Temp",
             partial.values = wind, partial.values2 = x2)
       cbind(wind, x2, get.partial.plot.data(o)$yhat)
     }))
pdta <- data.frame(pdta)
colnames(pdta) <- c("wind", "temp", "effectSize")

## coplot of partial effect of wind and temp
coplot(effectSize ~ wind|temp, pdta, pch = 16, overlap = 0)
```
